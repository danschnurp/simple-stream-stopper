from typing import Generator, Dict, Set
from collections import defaultdict, deque


class StopWordAutomaton:
    """
    Efektivn√≠ autom√°t pro detekci stop slov pomoc√≠ n-gram≈Ø.
    Pou≈æ√≠v√° trie strukturu pro rychl√© vyhled√°v√°n√≠ prefix≈Ø.
    """

    def __init__(self, stop_words: list[str]):
        self.stop_words = stop_words
        self.max_length = max(len(word) for word in stop_words) if stop_words else 0

        # Trie struktura pro prefixov√© vyhled√°v√°n√≠
        self.trie = {}
        self.complete_words = set(stop_words)

        # Sestav√≠me trie
        for word in stop_words:
            current = self.trie
            for char in word:
                if char not in current:
                    current[char] = {}
                current = current[char]
            current['$'] = True  # Znaƒçka konce slova

    def find_stop_word_at_position(self, text: str, start_pos: int) -> tuple[str, int] | None:
        """
        Najde stop slovo zaƒç√≠naj√≠c√≠ na dan√© pozici.
        Vrac√≠ (stop_word, end_position) nebo None.
        """
        if start_pos >= len(text):
            return None

        current = self.trie
        for i in range(start_pos, min(start_pos + self.max_length, len(text))):
            char = text[i]
            if char not in current:
                return None
            current = current[char]
            if '$' in current:
                # Na≈°li jsme kompletn√≠ stop slovo
                stop_word = text[start_pos:i + 1]
                return stop_word, i + 1

        return None

    def find_earliest_stop_word(self, text: str) -> tuple[str, int, int] | None:
        """
        Najde nejd≈ô√≠ve se vyskytuj√≠c√≠ stop slovo v textu.
        Vrac√≠ (stop_word, start_pos, end_pos) nebo None.
        """
        earliest_pos = len(text)
        earliest_word = None
        earliest_end = None

        for i in range(len(text)):
            result = self.find_stop_word_at_position(text, i)
            if result and i < earliest_pos:
                earliest_pos = i
                earliest_word = result[0]
                earliest_end = result[1]

        return (earliest_word, earliest_pos, earliest_end) if earliest_word else None


def cut_stream_stop_words(token_stream: Generator[str, None, None],
                          stop_words: list[str]) -> Generator[str, None, None]:
    """
    Optimalizovan√° funkce pou≈æ√≠vaj√≠c√≠ n-gramov√Ω autom√°t pro rychlou detekci stop slov.
    """
    if not stop_words:
        yield from token_stream
        return

    # Inicializujeme autom√°t pro stop slova
    automaton = StopWordAutomaton(stop_words)

    # Sliding window buffer pro efektivn√≠ zpracov√°n√≠
    buffer = ""
    token_queue = deque()  # (token, start_pos_in_buffer)

    for token in token_stream:
        start_pos = len(buffer)
        buffer += token
        token_queue.append((token, start_pos))

        # Zkontroluj stop slova pomoc√≠ automatu
        stop_result = automaton.find_earliest_stop_word(buffer)

        if stop_result:
            stop_word, stop_start, stop_end = stop_result

            # Vydej tokeny/ƒç√°sti token≈Ø p≈ôed stop slovem
            while token_queue:
                curr_token, token_start = token_queue.popleft()
                token_end = token_start + len(curr_token)

                if token_end <= stop_start:
                    # Cel√Ω token je p≈ôed stop slovem
                    yield curr_token
                elif token_start < stop_start:
                    # Token ƒç√°steƒçnƒõ prekr√Ωv√° stop slovo
                    prefix_len = stop_start - token_start
                    yield curr_token[:prefix_len]
                    break
                else:
                    # Token je za stop slovem, nevyd√°vej ho
                    break

            return

        # Optimalizace: vydej tokeny, kter√© u≈æ nem≈Ø≈æou obsahovat zaƒç√°tek stop slova
        safe_boundary = len(buffer) - automaton.max_length + 1

        emitted_tokens = []
        while token_queue:
            curr_token, token_start = token_queue[0]
            token_end = token_start + len(curr_token)

            if token_end <= safe_boundary:
                emitted_tokens.append(token_queue.popleft()[0])
            else:
                break

        # Vydej bezpeƒçn√© tokeny
        for token in emitted_tokens:
            yield token

        # Aktualizuj buffer a p≈ôepoƒç√≠tej pozice
        if emitted_tokens:
            emitted_length = sum(len(t) for t in emitted_tokens)
            buffer = buffer[emitted_length:]

            # P≈ôepoƒç√≠tej pozice zb√Ωvaj√≠c√≠ch token≈Ø
            updated_queue = deque()
            for token, pos in token_queue:
                updated_queue.append((token, pos - emitted_length))
            token_queue = updated_queue

    # Vydej v≈°echny zb√Ωvaj√≠c√≠ tokeny
    while token_queue:
        yield token_queue.popleft()[0]


# Pokroƒçil√© testov√°n√≠ s realistick√Ωmi sc√©n√°≈ôi
def test_enhanced_algorithm():
    print("=== ENHANCED ALGORITHM TESTS ===\n")

    print("TEST 1: Z√°kladn√≠ p≈ô√≠klad z √∫lohy")

    def gen1():
        for token in ["Ah", "oj, ", "sv", "ete", "!"]:
            yield token

    result1 = list(cut_stream_stop_words(gen1(), ["svet"]))
    print(f"V√Ωsledek: {result1}")
    print(f"Spojeno: '{(''.join(result1))}'")
    print(f"Oƒçek√°v√°no: 'Ahoj, ' (zastav√≠ se p≈ôed 'svet')\n")

    print("TEST 2: Realistick√Ω text stream - novinov√Ω ƒçl√°nek")

    def realistic_news_stream():
        # Simulace tokeriz√°toru, kter√Ω m≈Ø≈æe rozdƒõlit text nep≈ôedv√≠datelnƒõ
        text = "Prezident ƒåesk√© republiky dnes podepsal nov√Ω z√°kon o digitalizaci. Zmƒõny se dotknou v≈°ech obƒçan≈Ø a budou platit od p≈ô√≠≈°t√≠ho roku. Vl√°da oƒçek√°v√° √∫spory v ≈ô√°du miliard korun."

        # R≈Øzn√© zp≈Øsoby tokenizace (BPE-like, n√°hodn√© d√©lky 1-10 znak≈Ø)
        import random
        random.seed(42)  # Pro reprodukovatelnost

        i = 0
        while i < len(text):
            # N√°hodn√° d√©lka tokenu 1-10 znak≈Ø
            token_len = random.randint(1, min(10, len(text) - i))
            yield text[i:i + token_len]
            i += token_len

    result2 = list(cut_stream_stop_words(realistic_news_stream(), ["digitalizaci", "obƒçan≈Ø"]))
    print(f"V√Ωsledek: {result2}")
    print(f"Spojeno: '{(''.join(result2))}'")
    print("Oƒçek√°v√°no: text a≈æ do prvn√≠ho stop slova\n")

    print("TEST 3: Subword tokenizace (BPE-like)")

    def subword_stream():
        # Simulace Byte-Pair Encoding tokenizace
        tokens = [
            "ƒå", "esk", "√°", " repub", "lika", " m√°", " bohat", "ou",
            " hist", "orii", ".", " Pra", "ha", " je", " kr√°s", "n√©",
            " mƒõs", "to", " s", " mnoha", " pam√°t", "kami", "."
        ]
        for token in tokens:
            yield token

    result3 = list(cut_stream_stop_words(subword_stream(), ["historie", "Praha"]))
    print(f"V√Ωsledek: {result3}")
    print(f"Spojeno: '{(''.join(result3))}'")
    print("Oƒçek√°v√°no: text a≈æ do 'historie' nebo 'Praha'\n")

    print("TEST 4: Streaming chat s emotikonami")

    def chat_stream():
        # Simulace real-time chat streamu
        messages = [
            "Ahoj", " v≈°ichni", " üòä", " Jak", " se", " m√°te", "?",
            " Dnes", " je", " kr√°sn√Ω", " den", " ‚òÄÔ∏è", " Tƒõ≈°√≠m", " se",
            " na", " v√≠kend", "!", " Zastavte", " pros√≠m", " spam", "."
        ]
        for msg in messages:
            yield msg

    result4 = list(cut_stream_stop_words(chat_stream(), ["spam", "reklama"]))
    print(f"V√Ωsledek: {result4}")
    print(f"Spojeno: '{(''.join(result4))}'")
    print("Oƒçek√°v√°no: text a≈æ do 'spam'\n")

    print("TEST 5: K√≥dov√Ω stream s koment√°≈ôi")

    def code_stream():
        # Tokenizace k√≥du (m≈Ø≈æe b√Ωt nepravideln√°)
        code_tokens = [
            "def", " process", "_data", "(", "data", "):", "\n    ",
            "# TODO", ":", " optimalizovat", " tento", " algoritmus", "\n    ",
            "for", " item", " in", " data", ":", "\n        ",
            "if", " item", ".", "is_", "valid", "():", "\n            ",
            "yield", " item", ".", "transform", "()"
        ]
        for token in code_tokens:
            yield token

    result5 = list(cut_stream_stop_words(code_stream(), ["TODO", "optimalizovat"]))
    print(f"V√Ωsledek: {result5}")
    print(f"Spojeno: '{(''.join(result5))}'")
    print("Oƒçek√°v√°no: k√≥d a≈æ do 'TODO' nebo 'optimalizovat'\n")

    print("TEST 6: Vysokofrekvenƒçn√≠ stream s mal√Ωmi tokeny")

    def high_freq_stream():
        # Simulace velmi rychl√©ho streamu s mal√Ωmi tokeny
        import itertools

        # Generuj text po jednotliv√Ωch znac√≠ch a kr√°tk√Ωch sekvenc√≠ch
        text = "Toto je test vysokofrekvenƒçn√≠ho streamu dat s mnoha mal√Ωmi tokeny a obƒçasn√Ωmi del≈°√≠mi sekvencemi."

        # Mix jednotliv√Ωch znak≈Ø a kr√°tk√Ωch n-gram≈Ø
        i = 0
        while i < len(text):
            if i % 7 == 0 and i + 3 < len(text):  # Obƒças del≈°√≠ token
                yield text[i:i + 3]
                i += 3
            else:  # Vƒõt≈°inou jednotliv√© znaky
                yield text[i]
                i += 1

    result6 = list(cut_stream_stop_words(high_freq_stream(), ["vysokofrekv", "test"]))
    print(f"Poƒçet token≈Ø: {len(result6)}")
    print(f"Spojeno: '{(''.join(result6))}'")
    print("Oƒçek√°v√°no: zastav√≠ se u prvn√≠ho stop slova\n")

    print("TEST 7: Multilingual stream")

    def multilingual_stream():
        # Mix jazyk≈Ø s r≈Øzn√Ωmi znakov√Ωmi sadami
        tokens = [
            "Hello", " world", "! ", "Ahoj", " svƒõte", "! ",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π", " –º–∏—Ä", "! ", "„Åì„Çì„Å´„Å°„ÅØ", "‰∏ñÁïå", "! ",
            "This", " is", " forbidden", " content", "."
        ]
        for token in tokens:
            yield token

    result7 = list(cut_stream_stop_words(multilingual_stream(), ["forbidden", "–º–∏—Ä"]))
    print(f"V√Ωsledek: {result7}")
    print(f"Spojeno: '{(''.join(result7))}'")
    print("Oƒçek√°v√°no: text a≈æ do prvn√≠ho stop slova\n")

    print("TEST 8: Performance test - velkoobjemov√Ω stream")

    def large_stream():
        # Simulace skuteƒçnƒõ velk√©ho streamu
        import time

        start_time = time.time()
        count = 0

        # Generuj 50k token≈Ø
        for i in range(50000):
            if i == 25000:  # Stop slovo v polovinƒõ
                yield "STOP_WORD"
            else:
                yield f"tok_{i % 100}"
            count += 1

        return count, time.time() - start_time

    result8 = list(cut_stream_stop_words(large_stream(), ["STOP_WORD"]))
    print(f"Poƒçet zpracovan√Ωch token≈Ø: {len(result8)}")
    print(f"Oƒçek√°v√°no: 25000 token≈Ø (zastav√≠ se u STOP_WORD)\n")


def benchmark_algorithm():
    """Benchmark pro porovn√°n√≠ v√Ωkonu"""
    import time

    print("=== BENCHMARK TESTS ===\n")

    # Test 1: R≈Øzn√© poƒçty stop slov
    print("Benchmark 1: Vliv poƒçtu stop slov na v√Ωkon")

    def generate_stream(size=10000):
        for i in range(size):
            yield f"token{i % 50}"
        yield "FINAL_STOP"

    for num_stops in [1, 5, 10, 20, 50]:
        stop_words = [f"stop{i}" for i in range(num_stops)] + ["FINAL_STOP"]

        start = time.time()
        result = list(cut_stream_stop_words(generate_stream(), stop_words))
        elapsed = time.time() - start

        print(f"  {num_stops:2d} stop slov: {elapsed:.4f}s, {len(result)} token≈Ø")

    print("\nBenchmark 2: Vliv d√©lky stop slov")

    for word_len in [5, 10, 20, 30, 50]:
        stop_words = ["x" * word_len, "FINAL_STOP"]

        start = time.time()
        result = list(cut_stream_stop_words(generate_stream(), stop_words))
        elapsed = time.time() - start

        print(f"  D√©lka {word_len:2d}: {elapsed:.4f}s, {len(result)} token≈Ø")


if __name__ == "__main__":
    test_enhanced_algorithm()
    print("\n" + "=" * 50 + "\n")
    benchmark_algorithm()